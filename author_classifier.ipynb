{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "I'll start with simple data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/train_set.csv', encoding='utf8', engine='python')\n",
    "print('Thr dataset has {} rows and {} columns'.format(len(df.index), len(df.columns)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['author'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dataset is pretty well balanced except it lacks tweets of Donald J. Trump. \n",
    "\n",
    "Let's look at some tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 280)\n",
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dataset contains different languages, hashtags, URLs, and mentions. All that seems important. That could be tough for many NLU techniques, but I'll try to deal with it.\n",
    "# The first approach\n",
    "I usually start working on a task by building a simple baseline estimator. I do it before going deep into data analysis and exploration. It helps me to understand task complexity.\n",
    "\n",
    "Obviously, tweets are key to this classification task. So I'll ignore metadata fields for now. \n",
    "\n",
    "To build sentence representations suitable for classification, I'll use my default approach, which is TFIDF vectors of tokenized and lemmatized sentences.\n",
    "\n",
    "I'll use RandomForest as my baseline estimator. It requires a rather small amount of time for learning and often works fine even without hyperparameters optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score\n",
    "from spacy.lang.en import English\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "nlp = English()\n",
    "\n",
    "clzs = {n: i for i, n in enumerate(df['author'].unique())}\n",
    "\n",
    "labels = list()\n",
    "tweets = list()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    labels.append(clzs[row['author']])\n",
    "    tweets.append(' '.join([w.lemma_ for w in nlp(row['tweet']) if w.is_stop==False]))\n",
    "\n",
    "\n",
    "labels = np.array(labels)\n",
    "tweets = np.array(tweets)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.1, random_state=10)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_train_predict = model.predict(X_train)\n",
    "print('Train set score {:.3f}'.format(precision_score(y_train, y_train_predict, average='micro')))\n",
    "\n",
    "X_test = vectorizer.transform(X_test)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "print('Test set score {:.3f}'.format(precision_score(y_test, test_predict, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got micro-precision ~70% from that estimator. It is lower then I could expect to get from my solution, but it's a good start.\n",
    "\n",
    "Now I'm ready to explore the metadata fields.\n",
    "\n",
    "# Utilizing the metadata fields\n",
    "Let's do some aggregations on the metadata fields to explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df.groupby('author')\n",
    "df_agg.agg({\n",
    "'year' : [np.min, np.max],\n",
    "     'day_of_week': 'nunique',\n",
    "     'is_retweet': [np.mean],\n",
    "     'has_hashtag': [np.mean],\n",
    "     'has_mentions': [np.mean],\n",
    "     'has_url': [np.mean],\n",
    "     'has_media': [np.mean],\n",
    "'source': 'nunique', 'lang': 'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retweets are absent in this dataset. But other fields could be useful. As we can see, Elon Mask is loyal to his iPhone, while Cristiano Ronaldo is a leader by the number of sources he uses. Also, we know that Elon Musk and Donald Trump rarely use hashtags while Sebastian Ruder never posts media.\n",
    "\n",
    "I see a problem with the year field here. I've checked the test set manually and found out that all tweets in the test set before 2012 seem to be authored by Cristiano Ronaldo. So it could help me to improve the score on the test set.\n",
    "\n",
    "I don't know whether lower limits of the year column here actually show when those people joined twitter, or it is a limitation of the dataset. If we were building a production-ready software system, I would be cautious using the year for classification.\n",
    "\n",
    "Now, I'm ready to utilize the metadata in my baseline estimator. Fortunately, I used a decision tree estimator and it's already suitable for heterogeneous data. I only have to map string fields to numbers and attach metadata to TFIDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import scipy \n",
    "\n",
    "langs = ['en', 'pt', 'und', 'tl', 'ht', 'in', 'sv', 'es', 'it', 'pl', 'hi', 'et', 'ca', 'de', 'fi', 'fr',\n",
    "             'ru', 'hu', 'nl', 'da', 'tr', 'lt', 'ro', 'eu', 'ja', 'cy', 'sl', 'is', 'no', 'cs']\n",
    "\n",
    "days_of_week = ['Mon', 'Sat', 'Fri', 'Wed', 'Tue', 'Sun', 'Thu']\n",
    "\n",
    "lang_mappings = {n: i for i, n in enumerate(langs)}\n",
    "dow_mappings = {n: i for i, n in enumerate(days_of_week)}\n",
    "\n",
    "metadata = np.vstack([df['source'],\n",
    "                          df['lang'].apply(lambda x: lang_mappings[x]).values,\n",
    "                          df['day_of_week'].apply(lambda x: dow_mappings[x]).values,\n",
    "                          df['has_hashtag'].astype(int).values,\n",
    "                          df['has_mentions'].astype(int).values,\n",
    "                          df['has_url'].astype(int).values,\n",
    "                          df['has_media'].astype(int).values,\n",
    "                          df['day'].values, df['month'].values,\n",
    "                          df['year'].values, df['hour'].values, df['minute'].values,\n",
    "                          df['second'].values, df['day_of_year'].values,\n",
    "                          df['week_of_year'].values]).T\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "train_index, test_index = next(split.split(tweets, labels))\n",
    "\n",
    "tweets_train, tweets_test = tweets[train_index], tweets[test_index]\n",
    "labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "metadata_train, metadata_test = metadata[train_index], metadata[test_index]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tweets_train = vectorizer.fit_transform(tweets_train)\n",
    "tweets_test = vectorizer.transform(tweets_test)\n",
    "\n",
    "X_train = scipy.sparse.hstack([tweets_train, metadata_train])\n",
    "X_test = scipy.sparse.hstack([tweets_test, metadata_test])\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "model.fit(X_train,labels_train)\n",
    "\n",
    "train_predict = model.predict(X_train)\n",
    "print('Train set score {:.3f}'.format(precision_score(labels_train, train_predict, average='micro')))\n",
    "\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "print('Test set score {:.3f}'.format(precision_score(labels_test, test_predict, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got 82% here. That's a significant improvement! \n",
    "\n",
    "Now I'm ready to select a more suitable model if anyone exists.\n",
    "\n",
    "# Model selection\n",
    "I've started with RandomForest from scikit-learn. The data is heterogeneous and high-dimensional. So a tree-based estimator is a right choice for that data. But the most efficient tree-based ensemble estimator right now is a gradient boosting tree. So I'll try to improve my scores by switching to CatBoost. CatBoost is highly optimized, easy to train. It doesn't require a GPU on inference. And it could be trained both on a CPU and a GPU. There're several popular gradient boosting tree algorithms alongside CatBoost (XGBoost, for example), but CatBoost is my default choice. \n",
    "\n",
    "I'm not very experienced with the optimization of hyperparameters of gradient boosting trees. So I'll use a grid search for that. Note that the following code could run up to a couple of hours on a GPU and up to a day on an average CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(task_type=\"GPU\")#Change it to CPU if you don't have a GPU\n",
    "grid = {'learning_rate': [0.1, 0.3, 0.6],\n",
    "        'depth': [6, 10],\n",
    "        'l2_leaf_reg': [0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "grid_search_result = model.grid_search(grid,\n",
    "                                       X=X_train,\n",
    "                                       y=labels_train,\n",
    "                                       plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search showed the best result with learning_rate=0.3, depth=10, l2_leaf_reg=0.05.\n",
    "\n",
    "Now I'm ready to train and test model with optimized hyperparameters. To be sure that my results are valid, I'll use 5-Fold cross-validation. Note that the following code could run up to an hour on a GPU and up to twelve hours on an average CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "prs = list()\n",
    "for idx, (train_index, test_index) in enumerate(skf.split(metadata, labels)):\n",
    "    print('K-Fold iteration {}'.format(idx+1))\n",
    "\n",
    "    tweets_train = tweets[train_index]\n",
    "    metadata_train = metadata[train_index]\n",
    "\n",
    "    tweets_test = tweets[test_index]\n",
    "    metadata_test = metadata[test_index]\n",
    "\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tweets_train_tfidf = vectorizer.fit_transform(tweets_train)\n",
    "    tweets_test_tfidf = vectorizer.transform(tweets_test)\n",
    "\n",
    "    X_train = scipy.sparse.hstack([tweets_train_tfidf, metadata_train])\n",
    "    X_test = scipy.sparse.hstack([tweets_test_tfidf, metadata_test])\n",
    "\n",
    "    model = CatBoostClassifier(learning_rate=0.3, depth=10, l2_leaf_reg=0.1, task_type=\"GPU\")\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    y_train_predict = model.predict(X_train)\n",
    "    print('Train set score {}'.format(precision_score(y_train, y_train_predict, average='micro')))\n",
    "\n",
    "    y_test_predict = model.predict(X_test)\n",
    "\n",
    "    pr = precision_score(y_test, y_test_predict, average='micro')\n",
    "    prs.append(pr)\n",
    "    print('Test set score {}'.format(pr))\n",
    "\n",
    "\n",
    "print(prs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got 90-91%. And I think that's an amazing result. And I'm ready to build the final model on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tweets_train_tfidf = vectorizer.fit_transform(tweets)\n",
    "X_train = scipy.sparse.hstack([tweets_train_tfidf, metadata])\n",
    "model = CatBoostClassifier(learning_rate=0.3, depth=10, l2_leaf_reg=0.1, task_type=\"GPU\")\n",
    "model.fit(X_train,labels)\n",
    "if os.path.exists('./sample_model'):\n",
    "    shutil.rmtree('./sample_model')\n",
    "os.makedirs('./sample_model')\n",
    "model.save_model('./sample_model/model.cbm')\n",
    "with open('./sample_model/vectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model analysis\n",
    "Let's explore the most important features of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join('./model_cb_full', 'vectorizer.pickle'), 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "    \n",
    "features = vectorizer.get_feature_names()\n",
    "metadata_features =     ['source',\n",
    "     'lang',\n",
    "     'day_of_week',\n",
    "     'has_hashtag',\n",
    "     'has_mentions',\n",
    "     'has_url',\n",
    "     'has_media',\n",
    "     'day', 'month',\n",
    "     'year', 'hour', 'minute',\n",
    "     'second', 'day_of_year',\n",
    "     'week_of_year']\n",
    "\n",
    "model = CatBoostClassifier(learning_rate=0.3, depth=10, l2_leaf_reg=0.1, task_type=\"GPU\")\n",
    "model.load_model(os.path.join('./model_cb_full', './model.cbm'))\n",
    "feature_importance = model.get_feature_importance(type= \"FeatureImportance\")\n",
    "important_features = np.argsort(-feature_importance)\n",
    "\n",
    "print([features[x] if x < len(features) else metadata_features[x-len(features)] for x in important_features[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turned out to be tractable. Metadata fields are the most important ones. The most important words are hashtags like kkw*, ellentube, theellenshow. The model learned to predict authors based on some salient features rather than some in-depth text analysis. And it works well. \n",
    "\n",
    "I don't think that any publicly available word embeddings or pre-trained BERT models could deal with that data significantly better. [Character-based CNNs](https://ieeexplore.ieee.org/document/7966145/citations?tabFilter=papers#citations) could do so. Using of [pretrained tweet2vec](https://github.com/bdhingra/tweet2vec) could worse trying. Models based on some combinations of word embeddings, character embeddings, and metadata (like in [BIDAF](https://arxiv.org/abs/1611.01603)) could be better. But it's a matter of long research with unpredictable results.\n",
    "\n",
    "I also think that it is possible to achieve better results with my model by improving tokenization techniques to work better with emojis, emoticons, URLs, and other none-linguistic textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}